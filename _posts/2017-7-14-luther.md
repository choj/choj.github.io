---
layout: post
title: Predicting Rock Climbing Route Ratings from MountainProject Pages
---

I attempted to predict star ratings for individual rock climbing routes using data from the respective route webpages. Users vote to rate each route 1-5 stars, depending on the level of enjoyment. A linear regression approach was used with data scraped from 1,926 climbing routes in Joshua Tree and Yosemite National Parks. The final model yielded an R2 = 0.552 and Adjusted R2 = 0.544 when tested across the entire data sample. With 5-fold cross validation, these values were R2 = 0.524 and Adjusted R2 = 0.450.

Data were scraped with BeautifulSoup in Python. Initially, the search function on MountainProject was used to produce lists of climbing routes with at least five star votes each. These lists were scraped to create a list of routes and associated URLs. The individual route pages were then scraped scraped for these features, using a combination of BeautifulSoup function and regular expressions:

*	AvgStar: average star rating
*	YDSGrade: difficulty of the climb, typical values: 5.4, 5.9, 5.10a
*	NumPitches: number of pitches or “sections” of the route
*	Height: vertical distance traveled
*	FAYear: First ascent year, year in which route was first established
*	LocText: Location text body
*	DescText: Description text body
*	ProText: Protection text body
*	LenLoc: Character length of Location text
*	LenDesc: Character length of Description text
*	LenPro: Character length of Protection text
*	MainArea: e.g. Joshua Tree National Park
*	SubArea: (within Main Area), e.g. Lost Horse Area
*	Keywords: crack slab face crux easy hard steep good bad thin awkward fun scary

All route pages were saved locally via the Python pickle function.

Data were pre-processing before analysis. Climbing grades were converted from the Yosemite Decimal System to standard decimal values from the International Rock Climbing Research Association: https://www.ircra.rocks/single-post/2016/09/12/Reporting-Grades-in-Climbing-Research. These translated grades were stored in column GradeIRCRA. Character lengths of 0 were replaced with 1 to avoid divide-by-zero errors. The two categorical features, Main Area and Sub Area, were converted into 1-hot dmatrices using patsy and added to the dataframe. Text values were parsed for keywords.

Analysis began with looking at pairplots to discern any possible correlations and trends between features. Height, LenDesc, LenPro, and NumPitches appeared to have highly skewed distributions. Height and NumPitches appeared linearly correlated, which is expected since a longer route requires more pitches. AvgStar had a rough positive correlation with GradeIRCRA. Harder climbs tend to be enjoyed more, though a confounding factor could be that only higher-experience climbers attempt such routes. Skewed distribution features were normalized by taking the natural logarithm, storing the resulting values in columns log_height, log_lendesc, log_lenpro, and log_numpitches.

As a first look at modeling, a linear regression model was fit to all features via statsmodels. This showed that the approach was feasible, with an R2 = 0.574. To further develop the model, I implemented feature scaling, feature reduction, and cross validation. I selected three types of regularized linear regression models to investigate: Lasso, Ridge, and ElasticNet. These yielded similar results for mean squared error and adjusted R2, and I believe the quality of the resulting models was very close. I selected Lasso for its tendency to produce sparser models, under the assumption that many of my features had low influence on the predictive model and were chosen speculatively. This was coupled with StandardScaler from sklearn.preprocessing to scale features before model fitting.

Feature selection was carried out by selecting features whose p-values were below 0.02 using f_select.f_regression inside a kfold loop. kfold was used to generate 5 training/testing sets. For each set, the model was fit and scored for MSE, R2, and Adjusted R2. Scores were averaged across the 5 sets to evaluate each iteration.

The final model is composed of 23 feature columns (a reduction of 50%) with an R2 = 0.552, and Adjusted R2 = 0.544, when tested against the entire dataset. For 5-fold cross-validation, these values were 0.524 and 0.450.

